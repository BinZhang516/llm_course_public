{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec3. LangChain\n",
    "\n",
    "\n",
    "**LangChain** is a framework for developing applications powered by language models. It provides abundant abstractions about langage models and sources of context (prompt instructions, few shot examples, content to ground its response in, etc.), which enable the user to easily **chain** these components together for developing awesome applications.\n",
    "\n",
    "In this lab, we will learn several key abstractions in LangChain and build an input-output customized AI-powered web-search application.\n",
    "\n",
    "### Reference \n",
    "1. [Langchain document](https://python.langchain.com/docs/introduction/)\n",
    "\n",
    "\n",
    "## 0. First thing first\n",
    "\n",
    "### 0.1 Dependencies and Keys\n",
    "  \n",
    "In addition to the Open AI keys, add the following keys to your .env file.\n",
    "\n",
    "- Serp api key:\n",
    "    ```\n",
    "    SERPAPI_API_KEY=\"YOURKEY\"\n",
    "    ```\n",
    "    The `SERPAPI_API_KEY` is for invoking the search engine, first register through this [web site](https://serpapi.com/).\n",
    "\n",
    "    After getting these two keys, set your keys as environment variables.\n",
    "- Langchain API key (for tracing)\n",
    "    ```\n",
    "    LANGCHAIN_TRACING_V2=\"true\"\n",
    "    LANGCHAIN_API_KEY=ls_xxxxxxxx\n",
    "    ```\n",
    "    To create a `LANGCHAIN_API_KEY`, you can register through the [LANGSMITH](https://docs.smith.langchain.com/)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have installed these dependencies in your image\n",
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv  \n",
    "import os  \n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "# os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "# os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL=\"qwen2.5-72b-instruct\"\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.environ.get(\"INFINI_API_KEY\")  # langchain use this environment variable to find the OpenAI API key\n",
    "OPENAI_BASE=os.environ.get(\"INFINI_BASE_URL\") # will be used to pass the OpenAI base URL to langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Key abstractions in LangChain\n",
    "\n",
    "| Abstracted Components | Input Type                                | Output Type           |\n",
    "|-----------------------|-------------------------------------------|-----------------------|\n",
    "| Prompt                | Dictionary                                | PromptValue           |\n",
    "| ChatModel             | string, list of messages or a PromptValue | string, ChatMessage   |\n",
    "| OutputParser          | The output of an LLM or ChatModel         | Depends on the parser |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The ChatModel\n",
    "\n",
    "`ChatModels` is a language model which takes a list of messages or a string as input and returns a message or a string.\n",
    "\n",
    "`ChatModel` provides two methods to interact with the user:\n",
    "\n",
    "- `predict`: takes in a string, returns a string.\n",
    "- `predict_messages`: takes in a list of messages, returns a message.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChatModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "chat_model = ChatOpenAI(\n",
    "    temperature=0, \n",
    "    model=CHAT_MODEL,\n",
    "    base_url=OPENAI_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an output utility to show the type and the content of the result\n",
    "def print_with_type(res):\n",
    "    print(f\"%s\" % (type(res)))\n",
    "    print(f\"%s\" % res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four roles in LangChain, and you can define your own custom roles.\n",
    "\n",
    "- `HumanMessage`: A ChatMessage coming from a human/user.\n",
    "- `AIMessage`: A ChatMessage coming from an AI/assistant.\n",
    "- `SystemMessage`: A ChatMessage coming from the system.\n",
    "- `FunctionMessage`: A ChatMessage coming from a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='Hello Xu Wei! Nice to meet you too! I\\'d be happy to tell you about LangChain.\\n\\nLangChain is a framework for developing applications that use large language models (LLMs) in a structured and efficient way. It\\'s designed to help developers build complex AI applications more easily by providing a set of tools and abstractions that handle the common tasks and challenges associated with using LLMs.\\n\\n### Key Features of LangChain\\n\\n1. **Modular Architecture**:\\n   - LangChain allows you to break down your application into smaller, reusable components. This makes it easier to manage and scale your project.\\n\\n2. **Chain of Thought**:\\n   - One of the core concepts in LangChain is the \"Chain of Thought\" (CoT). This involves breaking down a task into a series of steps, each of which can be handled by a different model or function. This approach helps in creating more robust and flexible applications.\\n\\n3. **Agents**:\\n   - LangChain supports the creation of agents that can interact with the environment, make decisions, and execute actions based on the outputs of language models. These agents can be used for tasks like web browsing, data retrieval, and more.\\n\\n4. **Memory Management**:\\n   - Managing the context and history of interactions is crucial for many applications. LangChain provides tools to handle memory efficiently, ensuring that the model has access to relevant information when making decisions.\\n\\n5. **Integration with Models**:\\n   - LangChain is compatible with a wide range of language models, including those from Hugging Face, Anthropic, and others. This flexibility allows you to choose the best model for your specific use case.\\n\\n6. **Prompt Engineering**:\\n   - Crafting effective prompts is essential for getting good results from language models. LangChain includes tools and best practices for prompt engineering, helping you to create prompts that elicit the desired responses.\\n\\n7. **Evaluation and Testing**:\\n   - LangChain provides utilities for evaluating and testing your models and chains. This includes metrics for performance, accuracy, and other important aspects of your application.\\n\\n8. **Community and Ecosystem**:\\n   - LangChain has a growing community of developers and users who contribute to the ecosystem. This means you can find a lot of resources, tutorials, and support as you build your applications.\\n\\n### Use Cases\\n\\n- **Chatbots**: Building conversational agents that can handle complex queries and tasks.\\n- **Content Generation**: Generating high-quality text content for articles, stories, and more.\\n- **Data Analysis**: Using language models to analyze and interpret data.\\n- **Automated Testing**: Creating automated tests for software and systems.\\n- **Personal Assistants**: Developing personal assistants that can perform a variety of tasks.\\n\\nIf you have any specific questions or areas of interest within LangChain, feel free to ask! I\\'m here to help!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 575, 'prompt_tokens': 29, 'total_tokens': 604, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-df6a93e4-34ed-4dc8-b6ff-a8231673acce-0' usage_metadata={'input_tokens': 29, 'output_tokens': 575, 'total_tokens': 604, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "qtext = \"hello! my name is xu wei, nice to meet you! could you tell me something about langchain\"\n",
    "\n",
    "messages = []\n",
    "messages.append(\n",
    "    HumanMessage(content=qtext)  # construct a human message\n",
    "    )\n",
    "res = chat_model.invoke(messages)  # invoke the chat model\n",
    "\n",
    "print_with_type(res)\n",
    "\n",
    "messages.append(res)  # append the result to the chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructors are tedious to use, and you can use the following more friendly API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content=\"I'm just here to help you out! What can I assist you with today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 29, 'total_tokens': 46, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-d937045c-e645-45a8-ad66-ace900f55b6c-0' usage_metadata={'input_tokens': 29, 'output_tokens': 17, 'total_tokens': 46, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# a simpler way to manage messages\n",
    "from langchain.memory import ChatMessageHistory\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")  # avoid using the constructor directly\n",
    "history.add_ai_message(\"whats up?\")\n",
    "history.add_user_message(\"nothing much, you?\")\n",
    "\n",
    "res = chat_model.invoke(history.messages)\n",
    "print_with_type(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content=\"LangChain has a wide range of applications across various industries and use cases. Here are some of the key areas where LangChain can be particularly useful:\\n\\n### 1. **Conversational Agents and Chatbots**\\n- **Customer Support**: Building chatbots that can handle customer inquiries, provide information, and resolve issues.\\n- **Virtual Assistants**: Creating virtual assistants that can perform tasks such as scheduling appointments, sending reminders, and managing to-do lists.\\n- **E-commerce**: Implementing chatbots to assist customers with product recommendations, order tracking, and returns.\\n\\n### 2. **Content Generation**\\n- **Article Writing**: Automating the generation of news articles, blog posts, and other written content.\\n- **Creative Writing**: Assisting writers with generating story ideas, character descriptions, and plot outlines.\\n- **Marketing Content**: Creating marketing copy, social media posts, and email campaigns.\\n\\n### 3. **Data Analysis and Insights**\\n- **Business Intelligence**: Using language models to analyze business data and generate insights.\\n- **Financial Analysis**: Automating the analysis of financial reports and market trends.\\n- **Research Assistance**: Helping researchers by summarizing papers, extracting key findings, and generating hypotheses.\\n\\n### 4. **Automated Testing and Quality Assurance**\\n- **Test Case Generation**: Automatically generating test cases for software applications.\\n- **Bug Reporting**: Identifying and reporting bugs in code.\\n- **Code Review**: Providing feedback and suggestions for improving code quality.\\n\\n### 5. **Educational Tools**\\n- **Tutoring Systems**: Developing tutoring systems that can provide personalized learning experiences.\\n- **Grading and Feedback**: Automating the grading of assignments and providing detailed feedback.\\n- **Curriculum Development**: Assisting in the development of educational content and curricula.\\n\\n### 6. **Legal and Compliance**\\n- **Contract Analysis**: Analyzing legal documents to identify key terms and potential issues.\\n- **Compliance Checking**: Ensuring that documents and processes comply with regulatory requirements.\\n- **Legal Research**: Assisting lawyers in conducting legal research and preparing briefs.\\n\\n### 7. **Healthcare**\\n- **Patient Care**: Building chatbots that can provide medical advice, symptom checking, and appointment scheduling.\\n- **Research and Development**: Assisting in the analysis of medical data and the development of new treatments.\\n- **Telemedicine**: Enhancing telemedicine platforms with conversational agents that can triage patients and provide initial assessments.\\n\\n### 8. **Customer Experience**\\n- **Personalization**: Using language models to personalize user experiences, such as recommending products or services.\\n- **Feedback Analysis**: Analyzing customer feedback to identify common issues and improve products or services.\\n- **Surveys and Polls**: Automating the creation and analysis of surveys and polls.\\n\\n### 9. **Gaming and Entertainment**\\n- **NPC Development**: Creating non-player characters (NPCs) in games that can engage in natural language conversations.\\n- **Storytelling**: Generating dynamic and interactive storylines for games and other entertainment applications.\\n- **Virtual Worlds**: Building immersive virtual worlds where characters and environments can respond to user input.\\n\\n### 10. **Scientific Research**\\n- **Data Interpretation**: Assisting scientists in interpreting large datasets and generating hypotheses.\\n- **Literature Review**: Automating the process of reviewing scientific literature and summarizing key findings.\\n- **Experiment Design**: Helping researchers design experiments and analyze results.\\n\\n### 11. **Enterprise Applications**\\n- **Workflow Automation**: Automating routine tasks and workflows within organizations.\\n- **Knowledge Management**: Building knowledge management systems that can store, retrieve, and update information.\\n- **Decision Support**: Providing decision support tools that can analyze data and provide recommendations.\\n\\n### 12. **Social Media and Community Management**\\n- **Content Moderation**: Automating the moderation of user-generated content to ensure compliance with community guidelines.\\n- **Engagement**: Creating bots that can engage with users on social media platforms, respond to comments, and manage community discussions.\\n\\n### 13. **IoT and Smart Devices**\\n- **Voice Assistants**: Integrating voice assistants into smart home devices to control appliances, answer questions, and provide information.\\n- **Device Interaction**: Enabling natural language interaction with IoT devices to perform tasks and provide updates.\\n\\nThese are just a few examples of the many applications of LangChain. The framework's flexibility and modular design make it suitable for a wide range of use cases, from simple chatbots to complex enterprise solutions. If you have a specific application in mind or need more details on any of these areas, feel free to let me know!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 928, 'prompt_tokens': 619, 'total_tokens': 1547, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-63b50be7-0a99-4654-9ee6-4143eeb1a155-0' usage_metadata={'input_tokens': 619, 'output_tokens': 928, 'total_tokens': 1547, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# remembering the chat history and context\n",
    "\n",
    "qtext = \"what is its application?\"\n",
    "messages.append(HumanMessage(content=qtext))  ## providing context of chat histroy\n",
    "res = chat_model.invoke(messages)\n",
    "print_with_type(res)\n",
    "messages.append(res)  ## remembers the histroy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prompt templates\n",
    "\n",
    "LangChain provides ``PromptTemplate`` to help with formatting the prompts. \n",
    "A ``PromptTemplate`` allows you to define a template with placeholders that can be filled in with specific values at runtime. \n",
    "This helps in creating dynamic and reusable prompts for different contexts and inputs.\n",
    "\n",
    "The most plain prompt is in the type of a ``string``. Usually, the prompt includes several different type of `Messages`, which contains the `role` and the plain prompt as `content`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Simple template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "What is a good name for a company that makes candies?\n"
     ]
    }
   ],
   "source": [
    "# Prompt Template\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "input_prompt = prompt.format(product=\"candies\")\n",
    "\n",
    "print_with_type(input_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Chinese.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Chat Template (a list of temlates in a chat prompt template)\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "# format chat message prompt\n",
    "sys_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", sys_template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "chat_input = chat_prompt.format_messages(input_language=\"English\", output_language=\"Chinese\", text=\"I love programming.\")\n",
    "\n",
    "print_with_type(chat_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Using template in the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Chinese.', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is its application?', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='它的应用是什么？' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 29, 'total_tokens': 33, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-387ed511-fba7-47d9-929b-38da36ace72d-0' usage_metadata={'input_tokens': 29, 'output_tokens': 4, 'total_tokens': 33, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# format messages with PromptTemplate with translator as an example\n",
    "messages = []\n",
    "chat_input = chat_prompt.format_messages(input_language=\"English\", output_language=\"Chinese\", text=qtext)\n",
    "print_with_type(chat_input)\n",
    "print_with_type(chat_model.invoke(chat_input))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Chinese.', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is its application?', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='它的应用是什么？' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 29, 'total_tokens': 33, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-2b3329c8-21c7-4dce-ad87-9485088c0427-0' usage_metadata={'input_tokens': 29, 'output_tokens': 4, 'total_tokens': 33, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "messages = chat_input + messages  ## the system message must be at the beginning\n",
    "print_with_type(messages)\n",
    "\n",
    "res = chat_model.invoke(messages)\n",
    "print_with_type(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Chaining Components together\n",
    "\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components. \n",
    "In LangChain, most of the above key abstraction components are `Runnable` objects, and we can **chain** them together to build awesome applications. \n",
    "\n",
    "LangChain makes the chainning powerful through **LangChain Expression Language (LCEL)**, which can support chainning in manners of:\n",
    "\n",
    "- Async, Batch, and Streaming Support: any chain constructed in LCEL can automatically have full synv, async, batch and streaming support. \n",
    "- Fallbacks: due to many factors like network connection or non-deterministic properties, your LLM applications need to handle errors gracefully. With LCEL, your can easily attach fallbacks any chain.\n",
    "- Parallelism: since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
    "- LangSmith Tracing Integration: (for debugging, see below).\n",
    "\n",
    "In this lab, we only demonstrate the simplest functional chainning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='{\"这是输入到聊天模型链和聊天提示的内容。\"}' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 39, 'total_tokens': 52, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-92bacf32-90a2-4242-b54c-6cd70c4f0194-0' usage_metadata={'input_tokens': 39, 'output_tokens': 13, 'total_tokens': 52, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# More abstractions: bundling prompt and the chat_model into a chain\n",
    "\n",
    "translate_chain = chat_prompt | chat_model\n",
    "qtext = \"this is input to a chain of chat model and chat prompt.\"\n",
    "out = translate_chain.invoke({\n",
    "    \"input_language\": \"English\", \n",
    "    \"output_language\": \"Chinese\", \n",
    "    \"text\": {qtext}\n",
    "    })\n",
    "print_with_type(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Output parser\n",
    "\n",
    "Language models output text, which is often unstructured and free-form. However, in many applications, you may need more structured information to work with, such as JSON, XML, or other formats. This is where output parsers come in.\n",
    "\n",
    "Output parsers are tools that transform the raw text output of language models into structured data formats. The motivation for using output parsers is to facilitate easier data manipulation, integration, and analysis. By converting text into structured formats, you can more effectively utilize the information in downstream applications, automate workflows, and ensure consistency in data handling.\n",
    "\n",
    "LangChain provides several commonly-used output parsers, including:\n",
    "- [JSONparser](https://python.langchain.com/docs/how_to/output_parser_json/): Converts text output into JSON format.\n",
    "- [XMLparser](https://python.langchain.com/docs/how_to/output_parser_xml/): Converts text output into XML format.\n",
    "- [YAMLparser](https://python.langchain.com/docs/how_to/output_parser_yaml/): Converts text output into YAML format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Simple output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "{\"这是传递给聊天模型链和聊天提示的输入。\"}\n"
     ]
    }
   ],
   "source": [
    "# a simple output parser\n",
    "# StdOutParser converts the chat message to a string.\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "stdoutchain = chat_prompt | chat_model | output_parser\n",
    "\n",
    "qtext = \"this is input to a chain of chat model and chat prompt.\"\n",
    "out=stdoutchain.invoke({\n",
    "    \"input_language\": \"English\", \n",
    "    \"output_language\": \"Chinese\", \n",
    "    \"text\": {qtext}\n",
    "    })\n",
    "print_with_type(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Advanced output parsers: from Results to a Python Object\n",
    "Here we demonstrate the powerful Json Outputparser as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'name': 'Wei Xu', 'publications': 'A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games, Learning to Predict Intensions of Driver Actions from Visual Observations, A Survey on Driving Scene Understanding: Environmental Context, Motion and Intention, etc.'}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Professor(BaseModel):\n",
    "    name: str = Field(description=\"name of the Professor\")\n",
    "    publications: str = Field(description=\"the string of the professor's publications, separated by comma\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Professor)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "professor_chain = prompt | chat_model | parser\n",
    "query = \"tell me about professor Wei Xu including his publications.\"\n",
    "output = professor_chain.invoke({\n",
    "    \"query\": {query}\n",
    "    })\n",
    "print_with_type(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x7fdf053bbc80> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fdf048de2d0> root_client=<openai.OpenAI object at 0x7fdf04954380> root_async_client=<openai.AsyncOpenAI object at 0x7fdf048dc680> model_name='qwen2.5-72b-instruct' temperature=0.0 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://cloud.infini-ai.com/maas/v1'\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# see how langchain organizes the input to construct the result.\n",
    "# you can do so by printing the input of the chat_model.\n",
    "print(chat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the list of papers lacks substantial information and contains many inaccuracies. \n",
    "\n",
    "This is because the model has no knowlege about Prof. Wei Xu.  \n",
    "\n",
    "We will now demonstrate how to address these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Adding more contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Allowing the model to search the web: Retrievers\n",
    "\n",
    "Many LLM applications require user-specific data that is not part of the model's training set, like the above example : )\n",
    "The primary way of accomplishing this is through **Retrieval Augmented Generation (RAG)**. In this process, external data is retrieved and then passed to the LLM when doing the generation step. `Retriever` is an interface that returns documents given an unstructured query, which is used to provide the related contents to LLMs\n",
    "\n",
    "LangChain provides all the building blocks for RAG applications - from simple to complex, including document loaders, text embedding models and web searches.  We will introduce these models in Lab 4.  Here, we introduce three very basic retrievers that does web search and some local file access.  \n",
    "\n",
    "- [web page](https://python.langchain.com/docs/how_to/document_loader_web/)\n",
    "- [load Json](https://python.langchain.com/docs/how_to/document_loader_json/)\n",
    "- [load PDF files](https://python.langchain.com/docs/how_to/document_loader_pdf/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SerpAPI is a widely used API to access search engine results. It allows you to scrape and parse search engine data, providing a way to retrieve up-to-date information from the web.\n",
    "\n",
    "In LangChain, SerpAPI can be integrated as a retriever to enhance the capabilities of language models by providing them with the latest information from the web. This is particularly useful for applications that require current data that is not part of the model's training set.\n",
    "\n",
    "By using SerpAPI with LangChain, you can perform web searches and feed the retrieved data into the language model to generate more accurate and contextually relevant responses.\n",
    "\n",
    "In the following sections, we will demonstrate how to set up and use SerpAPI within LangChain to perform web searches and integrate the results into your language model workflows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'title': 'NVIDIA DGX Spark Arrives for World’s AI Developers', 'link': 'https://nvidianews.nvidia.com/news/nvidia-dgx-spark-arrives-for-worlds-ai-developers', 'source': 'NVIDIA Newsroom', 'source_logo': 'https://serpapi.com/searches/68ee2e33e827077519683a52/images/1facaa3a5b60e24b5fef4458f11d580e47b71da46047b6f88670150a7df2dbf5.png', 'date': '11 hours ago', 'thumbnail': 'https://serpapi.com/searches/68ee2e33e827077519683a52/images/1facaa3a5b60e24b43c2f81805cf6b5b3be5815be8f76320.jpeg'}, {'title': 'Elon Musk Gets Just-Launched NVIDIA DGX Spark: Petaflop AI Supercomputer Lands at SpaceX', 'link': 'https://blogs.nvidia.com/blog/live-dgx-spark-delivery/', 'source': 'NVIDIA Blog', 'source_logo': 'https://serpapi.com/searches/68ee2e33e827077519683a52/images/1facaa3a5b60e24beb289f690a0d6f0d88a3a365efb28929fe4b571d7ae4fb18.png', 'date': '5 hours ago', 'thumbnail': 'https://serpapi.com/searches/68ee2e33e827077519683a52/images/1facaa3a5b60e24b4c7b0aaf8781d047769a910f60ccdbb5.jpeg'}, {'title': 'Nvidia starts shipping $3,999 Blackwell-based desktop supercomputer', 'link': 'https://www.sdxcentral.com/news/nvidia-starts-shipping-3999-blackwell-based-desktop-supercomputer/', 'source': 'SDxCentral', 'source_logo': 'https://serpapi.com/searches/68ee2e33e827077519683a52/images/1facaa3a5b60e24b24eccfedc3f6477ceef961cb837078dd7742be8be9ceb859.png', 'date': '6 minutes ago', 'thumbnail': 'https://serpapi.com/searches/68ee2e33e827077519683a52/images/1facaa3a5b60e24bc3f521adf0ee9f31107994e50c988a68.jpeg'}, {'title': 'Navitas Semiconductor (NVTS): Assessing Valuation After NVIDIA AI Partnership and Manufacturing Expansion', 'link': 'https://finance.yahoo.com/news/navitas-semiconductor-nvts-assessing-valuation-080718260.html', 'source': 'Yahoo Finance', 'source_logo': 'https://serpapi.com/searches/68ee2e33e827077519683a52/images/1facaa3a5b60e24b3f5cc9ca76340abf28d444fc87a37fce3180126d3d419161.png', 'date': '2 days ago', 'thumbnail': 'https://serpapi.com/searches/68ee2e33e827077519683a52/images/1facaa3a5b60e24bfae7f5d60d411131c3c58ec69e5546a1.jpeg'}, {'title': 'Navitas Semiconductor Stock (NVTS): Soars on Nvidia Deal—But Can It Deliver?', 'link': 'https://www.valuethemarkets.com/analysis/navitas-semiconductor-stock-nvts-ai-data-center-boost', 'source': 'Value The Markets', 'source_logo': 'https://serpapi.com/searches/68ee2e33e827077519683a52/images/1facaa3a5b60e24b69e1a1e8aed316bd7e3fe2a81616e1db9dadee78418749c9.png', 'date': '20 minutes ago', 'thumbnail': 'https://serpapi.com/searches/68ee2e33e827077519683a52/images/1facaa3a5b60e24babd727336341ea3db8d3f4b7e073af98.jpeg'}, {'title': 'TSMC Just Shared Fantastic News for Nvidia and Broadcom Investors', 'link': 'https://www.fool.com/investing/2025/10/14/tsmc-just-shared-fantastic-news-nvidia-broadcom/', 'source': 'The Motley Fool', 'source_logo': 'https://serpapi.com/searches/68ee2e33e827077519683a52/images/1facaa3a5b60e24b75ed971bfdec17d530d4dd0952c00efa6d02d219db68ae25.png', 'date': '34 minutes ago', 'thumbnail': 'https://serpapi.com/searches/68ee2e33e827077519683a52/images/1facaa3a5b60e24b66fb3dbf7d1dd2bb5aaa3616b9d0b852.jpeg'}, {'title': \"Nvidia 800 VDC power rollout for 1 megawatt server racks to be supported by ABB — company says collaboration will create 'new power solutions for future gigawatt-scale data centers '\", 'link': 'https://www.tomshardware.com/tech-industry/big-tech/nvidia-800-vdc-power-rollout-for-1-megawatt-server-racks-to-be-supported-by-abb-company-says-collaboration-will-create-new-power-solutions-for-future-gigawatt-scale-data-centers', 'source': \"Tom's Hardware\", 'source_logo': 'https://serpapi.com/searches/68ee2e33e827077519683a52/images/1facaa3a5b60e24bebf192847f989b48e4881b44e365661eedd8ee701d05c5d4.png', 'date': '46 minutes ago', 'thumbnail': 'https://serpapi.com/searches/68ee2e33e827077519683a52/images/1facaa3a5b60e24b8138a12753d5821b3cc4677d8b5018de.jpeg'}]\n"
     ]
    }
   ],
   "source": [
    "# Using the search API\n",
    "\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "results = search.run(\"Nvidia\")\n",
    "print_with_type(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the search and LLM together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'title': 'NVIDIA Corporation Overview', 'brief_desc': 'NVIDIA, founded in 1993 by Jensen Huang, is a leading American technology company headquartered in Santa Clara, California. It is renowned for its invention of the GPU (Graphics Processing Unit) in 1999, which revolutionized the PC gaming market. NVIDIA specializes in designing advanced chips, systems, and software for AI, HPC, gaming, creative design, autonomous vehicles, and robotics. The company provides high-end GPUs and compute solutions globally.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "class News(BaseModel):\n",
    "    title: str = Field(description=\"title of the news\")\n",
    "    brief_desc: str = Field(description=\"brief descrption of the corresponding news\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=News)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query based on the following context: \\n{context}\\n{format_instructions}\\nQuery: {query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chat_model.temperature = 0\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "setup_and_retrieval = {\n",
    "        \"context\": search.run,  # passing a retriever\n",
    "        \"query\": RunnablePassthrough()\n",
    "}\n",
    "websearch_chain = setup_and_retrieval | prompt | chat_model | parser\n",
    "\n",
    "res = websearch_chain.invoke(\"tell me about the nvidia companies, and write a brief summary for it\")\n",
    "\n",
    "print_with_type(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Debugging and Logging\n",
    "\n",
    "#### 2.2.1 The debug mode\n",
    "\n",
    "LangChain provides a ``debug`` mode that allows you to see detailed information about the execution of your chains and agents. When debug mode is enabled, LangChain will print verbose output showing:\n",
    "\n",
    "1. The exact prompts being sent to the LLM\n",
    "2. The raw responses received from the LLM\n",
    "3. The execution flow of chains and agents\n",
    "4. Any intermediate steps and tool calls\n",
    "\n",
    "This is extremely useful for:\n",
    "- Troubleshooting unexpected outputs\n",
    "- Understanding how your chains are processing data\n",
    "- Optimizing prompts\n",
    "- Identifying errors in your chain's logic\n",
    "\n",
    "You can enable debug mode using `set_debug(True)` and disable it with `set_debug(False)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:run] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:run] [2.01s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"['NVIDIA engineers the most advanced chips, systems, and software for the AI factories of the future. We build new AI services that help companies create their ...', 'Founded in 1993, NVIDIA is the world leader in accelerated computing. Our invention of the GPU in 1999 sparked the growth of the PC gaming market,.', 'Nvidia Corporation is an American technology company headquartered in Santa Clara, California. Founded in 1993 by Jensen Huang (president and CEO), ...', 'NVIDIA Corporation (NVDA) is an American semiconductor company and a leading global manufacturer of high-end graphics processing units (GPUs).', 'Nvidia Corporation is a technology company known for designing and manufacturing graphics processing units (GPUs).', 'NVIDIA Corporation, a computing infrastructure company, provides graphics and compute and networking solutions in the United States, Singapore, Taiwan, China, ...', 'NVIDIA Corp (NVIDIA) is a designer and developer of graphics processing units, central processing units, and system-on-a-chip units.', 'The California-based chip designer, whose products are manufactured in Taiwan, is helping lay the foundation for the AI revolution.', 'NVIDIA Corp. engages in the design and manufacture of computer graphics processors, chipsets, and related multimedia software.', 'NVIDIA is engaged in manufacturing graphics processing units (GPUs) for the gaming and professional markets, as well as system-on-a-chips (SoCs) for mobile ...']\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query>] [2.02s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": \"['NVIDIA engineers the most advanced chips, systems, and software for the AI factories of the future. We build new AI services that help companies create their ...', 'Founded in 1993, NVIDIA is the world leader in accelerated computing. Our invention of the GPU in 1999 sparked the growth of the PC gaming market,.', 'Nvidia Corporation is an American technology company headquartered in Santa Clara, California. Founded in 1993 by Jensen Huang (president and CEO), ...', 'NVIDIA Corporation (NVDA) is an American semiconductor company and a leading global manufacturer of high-end graphics processing units (GPUs).', 'Nvidia Corporation is a technology company known for designing and manufacturing graphics processing units (GPUs).', 'NVIDIA Corporation, a computing infrastructure company, provides graphics and compute and networking solutions in the United States, Singapore, Taiwan, China, ...', 'NVIDIA Corp (NVIDIA) is a designer and developer of graphics processing units, central processing units, and system-on-a-chip units.', 'The California-based chip designer, whose products are manufactured in Taiwan, is helping lay the foundation for the AI revolution.', 'NVIDIA Corp. engages in the design and manufacture of computer graphics processors, chipsets, and related multimedia software.', 'NVIDIA is engaged in manufacturing graphics processing units (GPUs) for the gaming and professional markets, as well as system-on-a-chips (SoCs) for mobile ...']\",\n",
      "  \"query\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"context\": \"['NVIDIA engineers the most advanced chips, systems, and software for the AI factories of the future. We build new AI services that help companies create their ...', 'Founded in 1993, NVIDIA is the world leader in accelerated computing. Our invention of the GPU in 1999 sparked the growth of the PC gaming market,.', 'Nvidia Corporation is an American technology company headquartered in Santa Clara, California. Founded in 1993 by Jensen Huang (president and CEO), ...', 'NVIDIA Corporation (NVDA) is an American semiconductor company and a leading global manufacturer of high-end graphics processing units (GPUs).', 'Nvidia Corporation is a technology company known for designing and manufacturing graphics processing units (GPUs).', 'NVIDIA Corporation, a computing infrastructure company, provides graphics and compute and networking solutions in the United States, Singapore, Taiwan, China, ...', 'NVIDIA Corp (NVIDIA) is a designer and developer of graphics processing units, central processing units, and system-on-a-chip units.', 'The California-based chip designer, whose products are manufactured in Taiwan, is helping lay the foundation for the AI revolution.', 'NVIDIA Corp. engages in the design and manufacture of computer graphics processors, chipsets, and related multimedia software.', 'NVIDIA is engaged in manufacturing graphics processing units (GPUs) for the gaming and professional markets, as well as system-on-a-chips (SoCs) for mobile ...']\",\n",
      "  \"query\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the user query based on the following context: \\n['NVIDIA engineers the most advanced chips, systems, and software for the AI factories of the future. We build new AI services that help companies create their ...', 'Founded in 1993, NVIDIA is the world leader in accelerated computing. Our invention of the GPU in 1999 sparked the growth of the PC gaming market,.', 'Nvidia Corporation is an American technology company headquartered in Santa Clara, California. Founded in 1993 by Jensen Huang (president and CEO), ...', 'NVIDIA Corporation (NVDA) is an American semiconductor company and a leading global manufacturer of high-end graphics processing units (GPUs).', 'Nvidia Corporation is a technology company known for designing and manufacturing graphics processing units (GPUs).', 'NVIDIA Corporation, a computing infrastructure company, provides graphics and compute and networking solutions in the United States, Singapore, Taiwan, China, ...', 'NVIDIA Corp (NVIDIA) is a designer and developer of graphics processing units, central processing units, and system-on-a-chip units.', 'The California-based chip designer, whose products are manufactured in Taiwan, is helping lay the foundation for the AI revolution.', 'NVIDIA Corp. engages in the design and manufacture of computer graphics processors, chipsets, and related multimedia software.', 'NVIDIA is engaged in manufacturing graphics processing units (GPUs) for the gaming and professional markets, as well as system-on-a-chips (SoCs) for mobile ...']\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"properties\\\": {\\\"title\\\": {\\\"description\\\": \\\"title of the news\\\", \\\"title\\\": \\\"Title\\\", \\\"type\\\": \\\"string\\\"}, \\\"brief_desc\\\": {\\\"description\\\": \\\"brief descrption of the corresponding news\\\", \\\"title\\\": \\\"Brief Desc\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"title\\\", \\\"brief_desc\\\"]}\\n```\\nQuery: tell me about the company nvidia, and write a brief summary for it\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [4.70s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n  \\\"title\\\": \\\"NVIDIA Corporation Overview\\\",\\n  \\\"brief_desc\\\": \\\"NVIDIA Corporation, founded in 1993 and headquartered in Santa Clara, California, is a leading technology company specializing in the design and manufacture of graphics processing units (GPUs) and other advanced computing technologies. Known for inventing the GPU in 1999, which revolutionized the PC gaming market, NVIDIA has since expanded its focus to include accelerated computing, AI, and system-on-a-chip (SoC) solutions for various industries. The company's innovations are pivotal in driving the future of artificial intelligence and high-performance computing.\\\"\\n}\\n```\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"```json\\n{\\n  \\\"title\\\": \\\"NVIDIA Corporation Overview\\\",\\n  \\\"brief_desc\\\": \\\"NVIDIA Corporation, founded in 1993 and headquartered in Santa Clara, California, is a leading technology company specializing in the design and manufacture of graphics processing units (GPUs) and other advanced computing technologies. Known for inventing the GPU in 1999, which revolutionized the PC gaming market, NVIDIA has since expanded its focus to include accelerated computing, AI, and system-on-a-chip (SoC) solutions for various industries. The company's innovations are pivotal in driving the future of artificial intelligence and high-performance computing.\\\"\\n}\\n```\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 133,\n",
      "                \"prompt_tokens\": 523,\n",
      "                \"total_tokens\": 656,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-425a291c-599e-448c-bf3a-d1cfaf78e6b5-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 523,\n",
      "              \"output_tokens\": 133,\n",
      "              \"total_tokens\": 656,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 133,\n",
      "      \"prompt_tokens\": 523,\n",
      "      \"total_tokens\": 656,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [22ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"title\": \"NVIDIA Corporation Overview\",\n",
      "  \"brief_desc\": \"NVIDIA Corporation, founded in 1993 and headquartered in Santa Clara, California, is a leading technology company specializing in the design and manufacture of graphics processing units (GPUs) and other advanced computing technologies. Known for inventing the GPU in 1999, which revolutionized the PC gaming market, NVIDIA has since expanded its focus to include accelerated computing, AI, and system-on-a-chip (SoC) solutions for various industries. The company's innovations are pivotal in driving the future of artificial intelligence and high-performance computing.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [6.75s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"title\": \"NVIDIA Corporation Overview\",\n",
      "  \"brief_desc\": \"NVIDIA Corporation, founded in 1993 and headquartered in Santa Clara, California, is a leading technology company specializing in the design and manufacture of graphics processing units (GPUs) and other advanced computing technologies. Known for inventing the GPU in 1999, which revolutionized the PC gaming market, NVIDIA has since expanded its focus to include accelerated computing, AI, and system-on-a-chip (SoC) solutions for various industries. The company's innovations are pivotal in driving the future of artificial intelligence and high-performance computing.\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{'title': 'NVIDIA Corporation Overview', 'brief_desc': \"NVIDIA Corporation, founded in 1993 and headquartered in Santa Clara, California, is a leading technology company specializing in the design and manufacture of graphics processing units (GPUs) and other advanced computing technologies. Known for inventing the GPU in 1999, which revolutionized the PC gaming market, NVIDIA has since expanded its focus to include accelerated computing, AI, and system-on-a-chip (SoC) solutions for various industries. The company's innovations are pivotal in driving the future of artificial intelligence and high-performance computing.\"}\n"
     ]
    }
   ],
   "source": [
    "# Debugging and logging: debug mode\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "\n",
    "# Try rerun the previous example to see the verbose output.\n",
    "res = websearch_chain.invoke(\"tell me about the company nvidia, and write a brief summary for it\")\n",
    "\n",
    "print_with_type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'title': 'NVIDIA Corporation Overview', 'brief_desc': \"NVIDIA, founded in 1993 and headquartered in Santa Clara, California, is a leading technology company specializing in accelerated computing. Known for inventing the GPU in 1999, which revolutionized the PC gaming market, NVIDIA now leads in high-performance graphics processing units (GPUs) and is expanding into AI, data centers, and autonomous vehicles. The company's innovations also include system-on-a-chip (SoC) solutions for mobile computing and advanced software for AI services.\"}\n"
     ]
    }
   ],
   "source": [
    "set_debug(False)\n",
    "# Try rerun the previous example to see the verbose output.\n",
    "res = websearch_chain.invoke(\"tell me about the company nvidia, and write a brief summary for it\")\n",
    "\n",
    "print_with_type(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Tracing with LangSmith\n",
    "\n",
    "LangSmith is a developer platform that helps you debug, test, evaluate, and monitor LLM applications. It provides:\n",
    "\n",
    "1. **Tracing**: Visualize the execution flow of your chains and agents\n",
    "2. **Debugging**: Inspect inputs, outputs, and intermediate steps\n",
    "3. **Evaluation**: Measure and compare model performance\n",
    "4. **Monitoring**: Track usage and performance in production\n",
    " \n",
    "To use LangSmith tracing:\n",
    "Set environment variables (in your .env file):\n",
    "   - `LANGCHAIN_TRACING_V2=\"true\"` to enable tracing\n",
    "   - `LANGCHAIN_API_KEY` with your LangSmith API key\n",
    "\n",
    "After running your code, you can view detailed traces at https://docs.smith.langchain.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'NVIDIA Corporation: A Leader in Accelerated Computing',\n",
       " 'brief_desc': 'NVIDIA, founded in 1993, is a leading American technology company headquartered in Santa Clara, California. Known for inventing the GPU in 1999, which revolutionized the PC gaming market, NVIDIA has since expanded its focus to include AI, HPC, autonomous vehicles, and robotics. The company designs and manufactures advanced chips, systems, and software, providing high-end graphics processing units (GPUs) for gaming, professional markets, and more.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "\n",
    "# Auto-trace LLM calls in-context\n",
    "client = wrap_openai(openai.Client())\n",
    "\n",
    "@traceable # Auto-trace this function\n",
    "def pipeline(user_input: str):\n",
    "    return websearch_chain.invoke(user_input)\n",
    "pipeline(\"tell me about the nvidia companies, and write a brief summary for it\")\n",
    "# Out:  Hello there! How can I assist you today? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your task ####\n",
    "# go to the langsmith webpage and observe the traces. \n",
    "\n",
    "# Trace可见，Trace ID: f52f2889-a538-4db1-bad4-99532869f887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'name': 'Wei Xu', 'affiliation': 'Tsinghua University', 'publications': 'A Survey on Dialogue Systems: Recent Advances and New Frontiers, Deep Reinforcement Learning-based Methods for Unsupervised Video Object Segmentation, Visual Dialog, Neural Response Generation with Dynamic Lexicon, Neural Network-based Models for Unsupervised Entity Linking, Learning to Ask: Question Generation from Pre-trained Language Models, Unsupervised Text Style Transfer using Language Models as Discriminators, Adversarial Ranking for Language Generation, Learning to Generate Product Reviews from Attributes, A Neural Influence Model for Personalized Recommendation, Context-aware Sequential Recommendation'}\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# retrieve the information and fix the query results about Prof. Xu, \n",
    "# generating the correct Professor object.\n",
    "# Note that you do not have to get a perfect answer from the LLM in this lab.  \n",
    "# (if the answer is not perfect, please analyze and debug it in the next cell.)\n",
    "class Professor_ext(BaseModel):\n",
    "    name: str = Field(description=\"name of the Professor\")\n",
    "    affiliation : str = Field(description=\"which univeristy the professor is working with\")\n",
    "    publications: str = Field(description=\"the string of the professor's publications, separated by comma\")\n",
    "parser = JsonOutputParser(pydantic_object=Professor_ext)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "search = SerpAPIWrapper()\n",
    "setup_and_retrieval = {\n",
    "        \"context\": search.run,  # passing a retriever\n",
    "        \"query\": RunnablePassthrough()\n",
    "}\n",
    "professor_chain = setup_and_retrieval | prompt | chat_model | parser\n",
    "query = \"tell me about professor Wei Xu including his top-10 publications.\"\n",
    "output = professor_chain.invoke({\n",
    "    \"query\": {query}\n",
    "    })\n",
    "print_with_type(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR TASK ####\n",
    "# analyze the answer, if the answer is not correct, write down some comments \n",
    "# about why the answer is not correct. \n",
    "# 现在的输出包含了Xu Wei老师的工作单位和Top 10的publicaiton，相比之前以后优化，若要进一步改进，还需更多的提示词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Smarter workflow: Agents\n",
    "\n",
    "\n",
    "An AI agent is an autonomous system that perceives its environment, makes decisions, and takes actions to achieve specific goals. In the context of LLMs, an agent uses a language model as its reasoning engine to determine what actions to take and in what order, unlike chains where the sequence is predefined.\n",
    "\n",
    "LangChain provides several frameworks for building agents:\n",
    "1. **Tool integration**: LangChain allows agents to use external tools and APIs to gather information or perform actions\n",
    "2. **Agent types**: Supports various agent architectures like ReAct (Reasoning and Acting), Plan-and-Execute, and others\n",
    "3. **Memory systems**: Enables agents to maintain context across interactions (next lab)\n",
    "4. **Structured output**: Helps parse and validate agent responses\n",
    "\n",
    "For more advanced agent capabilities, LangGraph (an extension of LangChain) offers enhanced features for creating highly controllable and customizable agents with better state management and complex workflows (next lab).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Function-calling: Letter r's in straberry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the following very simple example, and see if LLM can get it correct. (the correct answer is 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The word \"strawberry\" contains two \\'r\\' letters.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 19, 'total_tokens': 33, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7d2690e8-f208-4a89-862a-a74a77419a18-0', usage_metadata={'input_tokens': 19, 'output_tokens': 14, 'total_tokens': 33, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"how many r's are there in the word strawberry?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the AI answered incorrectly by stating there are two 'r's in the word \"strawberry\" when there are actually three 'r's (st**r**awbe**rr**y). This demonstrates a limitation of LLMs in performing simple counting tasks. Even advanced models can make these basic errors because they process text holistically rather than character-by-character like humans do. This is why tools like function calling are useful - they allow us to delegate specific tasks (like counting) to dedicated functions that can perform them accurately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[StructuredTool(name='get_letter_count', description='Returns the number of a specific letter in a word. \\n    The input should be in the format: word,letter (e.g., strawberry,r)', args_schema=<class 'langchain_core.utils.pydantic.get_letter_count'>, func=<function get_letter_count at 0x7fdebc4a7420>)]\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "# here is an example of a tool that can be used to count the number of a specific letter in a word.\n",
    "# note that we only use a single string parameter, because the simple agent only accept tools with a single parameter.\n",
    "# to fit two parameters, we need a clear format instruction for the input.\n",
    "@tool\n",
    "def get_letter_count(query: str) -> int:\n",
    "    \"\"\"Returns the number of a specific letter in a word. \n",
    "    The input should be in the format: word,letter (e.g., strawberry,r)\"\"\"\n",
    "    word, letter = query.split(',')\n",
    "    return word.count(letter)\n",
    "\n",
    "print(get_letter_count.invoke(\"strawberry,r\"))\n",
    "\n",
    "tools = [ get_letter_count ]\n",
    "\n",
    "print(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ( \"system\", \"You are very powerful assistant who can use tools, but bad at counting letters in words.\", \n",
    "         ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32295/2941073740.py:5: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_chain = initialize_agent(tools,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"how many r's are there in the word strawberry?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"how many r's are there in the word strawberry?\",\n",
      "  \"agent_scratchpad\": \"\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nget_letter_count(query: str) -> int - Returns the number of a specific letter in a word. \\n    The input should be in the format: word,letter (e.g., strawberry,r)\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [get_letter_count]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: how many r's are there in the word strawberry?\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [2.47s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I need to count the number of 'r' letters in the word \\\"strawberry\\\".\\nAction: get_letter_count\\nAction Input: strawberry,r\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I need to count the number of 'r' letters in the word \\\"strawberry\\\".\\nAction: get_letter_count\\nAction Input: strawberry,r\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 64,\n",
      "                \"prompt_tokens\": 180,\n",
      "                \"total_tokens\": 244,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-b543039f-f026-4e5f-bad2-e06f86665ee8-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 180,\n",
      "              \"output_tokens\": 64,\n",
      "              \"total_tokens\": 244,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 64,\n",
      "      \"prompt_tokens\": 180,\n",
      "      \"total_tokens\": 244,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [2.47s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I need to count the number of 'r' letters in the word \\\"strawberry\\\".\\nAction: get_letter_count\\nAction Input: strawberry,r\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:get_letter_count] Entering Tool run with input:\n",
      "\u001b[0m\"strawberry,r\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:get_letter_count] [3ms] Exiting Tool run with output:\n",
      "\u001b[0m\"3\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"how many r's are there in the word strawberry?\",\n",
      "  \"agent_scratchpad\": \"I need to count the number of 'r' letters in the word \\\"strawberry\\\".\\nAction: get_letter_count\\nAction Input: strawberry,r\\nObservation: 3\\nThought:\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nget_letter_count(query: str) -> int - Returns the number of a specific letter in a word. \\n    The input should be in the format: word,letter (e.g., strawberry,r)\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [get_letter_count]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: how many r's are there in the word strawberry?\\nThought:I need to count the number of 'r' letters in the word \\\"strawberry\\\".\\nAction: get_letter_count\\nAction Input: strawberry,r\\nObservation: 3\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [1.05s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I now know the final answer\\nFinal Answer: There are 3 'r's in the word strawberry.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I now know the final answer\\nFinal Answer: There are 3 'r's in the word strawberry.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 22,\n",
      "                \"prompt_tokens\": 218,\n",
      "                \"total_tokens\": 240,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-5cfe6942-80e3-44f6-9188-d759e71205a7-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 218,\n",
      "              \"output_tokens\": 22,\n",
      "              \"total_tokens\": 240,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 22,\n",
      "      \"prompt_tokens\": 218,\n",
      "      \"total_tokens\": 240,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [1.06s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I now know the final answer\\nFinal Answer: There are 3 'r's in the word strawberry.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor] [3.54s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"There are 3 'r's in the word strawberry.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "agent_chain = initialize_agent(tools, \n",
    "                               chat_model, \n",
    "                               agent=\"zero-shot-react-description\", \n",
    "                               prompt_template=prompt, \n",
    "                               verbose=False\n",
    "                               )\n",
    "\n",
    "agent_chain.invoke({\"input\": \"how many r's are there in the word strawberry?\"})\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[StructuredTool(name='get_number_sorted', description='sort an array of numbers \\n        The input should be in the format: num1,num2,num3,...)', args_schema=<class 'langchain_core.utils.pydantic.get_number_sorted'>, func=<function get_number_sorted at 0x7fdddaa20900>)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'sort the numbers: 6,7,1,4,2,5,3,9,8',\n",
       " 'output': 'The sorted numbers are 1, 2, 3, 4, 5, 6, 7, 8, 9.'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### your task ####\n",
    "# implement a tool that sort an array of numbers (packed as a comma separated string)\n",
    "# then ask the agent to use the tool to sort an input array. \n",
    "@tool\n",
    "def get_number_sorted(query: str) -> list[int]:\n",
    "    \"\"\"sort an array of numbers \n",
    "        The input should be in the format: num1,num2,num3,...)\"\"\"\n",
    "    # prefix, words = query.split(':')\n",
    "    nums = query.split(',')\n",
    "    res = []\n",
    "    for num in nums :\n",
    "        res.append(int(num))\n",
    "    res.sort()\n",
    "    return res\n",
    "nums = \"6,7,1,4,2,5,3,9,8\"\n",
    "\n",
    "print(get_number_sorted.invoke(nums))\n",
    "\n",
    "tools = [ get_number_sorted ]\n",
    "\n",
    "print(tools)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ( \"system\", \"You are very powerful assistant who can use tools, but bad at sorting numbers in strings.\", \n",
    "         ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n",
    "    ]\n",
    ")\n",
    "\n",
    "agent_chain = initialize_agent(tools, \n",
    "                               chat_model, \n",
    "                               agent=\"zero-shot-react-description\", \n",
    "                               prompt_template=prompt, \n",
    "                               verbose=False\n",
    "                               )\n",
    "\n",
    "agent_chain.invoke({\"input\": \"sort the numbers: 6,7,1,4,2,5,3,9,8\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create an auto-web-search AI Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the following example about the built-in web-search tool and understand the code. \n",
    "\n",
    "\n",
    "from langchain.agents import load_tools \n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=News)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, helping the users search the web and write summary for the user's interested topic: {keyword}\",\n",
    "        ),\n",
    "        (\"user\", \"{keyword}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "tools = [load_tools([\"serpapi\"], chat_model)[0]]\n",
    "\n",
    "agent_chain = initialize_agent(tools, chat_model, agent=\"zero-shot-react-description\", prompt_template=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find recent news from Tsinghua University.\n",
      "Action: Search\n",
      "Action Input: \"Tsinghua University news last week\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['\\u200bCelebrating 100 years of Tsinghua Academy of Chinese Learning · \\u200bThe opening ceremony of Chinese Cultural Subjectivity, World Vision: Centennial Symposium ...', \"LATEST NEWS · \\u200bTsinghua's Long Di awarded by AGU for contributions to hydrologic sciences · From Tsinghua to the field: A Tsinghua intern at UNHCR Zambia · \\u200b ...\", 'The latest news, analysis and opinion on Tsinghua University. In-depth analysis, industry insights and expert opinion.', 'A House select committee is requesting more information about a university collaboration that it said could help China gain access to cutting-edge research.', 'LATEST NEWS · \\u200bTsinghua and PKU jointly triumph at 16th S.-T. · \\u200b13th World Peace Forum opens at Tsinghua, Chinese vice president addresses the forum · \\u200b2025 ...', 'A joint team from Tsinghua University and the University of Chile took the top prize with a solar- powered system that supports rural water ...', '(ECNS) -- Apple CEO Tim Cook on Tuesday announced a new donation to Tsinghua University aimed at supporting student development in sustainable ...', 'The first round of undergrad and art programs applications is open now! Tsinghua offers 93 undergraduate majors & 44 minors across 12 disciplines, ...', \"Influential maths expert calls to nurture domestic talent, notes 'profound symbolic weight' of Hong Kong's bid to host top 2030 conference.\", 'Tsinghua SEM hosted a summer program in 2025, held a welcome event for MBA exchange students, and hosted the 20th AAPBS Annual Meeting in 2024.']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have gathered several pieces of recent news from Tsinghua University.\n",
      "Final Answer: Within the last week, notable news from Tsinghua University includes:\n",
      "1. The opening ceremony of the Centennial Symposium celebrating 100 years of the Tsinghua Academy of Chinese Learning.\n",
      "2. Tsinghua's Long Di was awarded by the American Geophysical Union (AGU) for contributions to hydrologic sciences.\n",
      "3. A joint team from Tsinghua University and the University of Chile won a competition with a solar-powered system designed to support rural water systems.\n",
      "4. Apple CEO Tim Cook announced a new donation to Tsinghua University to support student development in sustainability.\n",
      "5. The World Peace Forum opened at Tsinghua University, where the Chinese vice president addressed the forum.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me the news from tsinghua university within last week?',\n",
       " 'output': \"Within the last week, notable news from Tsinghua University includes:\\n1. The opening ceremony of the Centennial Symposium celebrating 100 years of the Tsinghua Academy of Chinese Learning.\\n2. Tsinghua's Long Di was awarded by the American Geophysical Union (AGU) for contributions to hydrologic sciences.\\n3. A joint team from Tsinghua University and the University of Chile won a competition with a solar-powered system designed to support rural water systems.\\n4. Apple CEO Tim Cook announced a new donation to Tsinghua University to support student development in sustainability.\\n5. The World Peace Forum opened at Tsinghua University, where the Chinese vice president addressed the forum.\"}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.invoke(\"tell me the news from tsinghua university within last week?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI need to find information on the top-10 publications of Prof. Wei Xu from Tsinghua University. The best way to get this information is through a web search.\n",
      "Action: Search\n",
      "Action Input: top-10 publications Prof. Wei Xu Tsinghua University\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['My current projects include privacy-preserving computation, data center networking, large scale system for machine learning and data mining, as well as various ...', 'Wei Xu. Professor, IIIS and CollegeAI, Tsinghua University. Verified email at tsinghua.edu.cn - Homepage · Computer Science. ArticlesCited byPublic access ...', 'I was a postdoctoral researcher at the University of Pennsylvania. I received my PhD in Computer Science from New York University, BSMS from Tsinghua University ...', 'Tsinghua University. Assistant Professor at Institute for Interdisciplinary Information Sciences IIIS. Research Area: Distributed Systems + Machine Learning.', 'Semi-supervised learning for neural machine translation. Y Cheng. Joint training for neural machine translation, 25-40, 2019. 330, 2019 ; Doc2EDAG: An End-to-End ...', \"Wei Xu's 25 research works with 135 citations, including: The Singapore Consensus on Global AI Safety Research Priorities.\", 'Wei XUE. Professor and Director of High Performance Computing Institute. Department of Computer Science and Technology. Joined Department: 2003.', 'Publications. Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking? Chengda Lu, Xiaoyu Fan, Yu Huang, Rongwu Xu, Jijie Li, Wei Xu.', 'Her research interests are in natural language processing, machine learning, and social media. Her recent work focuses on text generation, stylistics, ...', 'Publications based on Top Specialties ; Activity recognition ; Human Action ; Neural network ; Pattern recognition ; Social network ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results provide some publications and research interests of individuals named Wei Xu, but it does not clearly list the top-10 publications of Prof. Wei Xu from Tsinghua University. I will need to refine the search to get more specific results.\n",
      "Action: Search\n",
      "Action Input: Prof. Wei Xu Tsinghua University top publications\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['I have a broad research interest in distributed system design, big data and financial technology. My current projects include privacy-preserving computation, ...', 'Wei Xu. Professor, IIIS and CollegeAI, Tsinghua University. Verified email at tsinghua.edu.cn - Homepage · Computer Science. ArticlesCited byPublic access ...', 'Research direction:Secure and robust AI application; AI Security; Privacy-preserving applications; High-performance distributed LLM training and inference.', 'Her research interests are in natural language processing, machine learning, and social media. Her recent work focuses on text generation, stylistics, ...', \"Wei Xu's 6 research works with 114 citations, including: Multi-object events recognition from video sequences using extended finite state machine.\", \"Wei Xu · Last year's Top subject · Last year's Top keyword · GORAM: Graph-Oriented ORAM for Efficient Ego-Centric Queries on Federated Graphs · IBGP: Imperfect ...\", 'I was a postdoctoral researcher at the University of Pennsylvania. I received my PhD in Computer Science from New York University, BSMS from Tsinghua University ...', 'Publications. Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking? Chengda Lu, Xiaoyu Fan, Yu Huang, Rongwu Xu, Jijie Li, Wei Xu.', 'Wei Xu. Professor, IIIS and CollegeAI, Tsinghua University. 在 tsinghua.edu.cn 的电子邮件经过验证 - 首页 · Computer Science. 文章引用次数开放获取的出版物 ...', \"Wei Xu's 25 research works with 135 citations, including: The Singapore Consensus on Global AI Safety Research Priorities.\"]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results still do not provide a clear list of the top-10 publications of Prof. Wei Xu from Tsinghua University. It seems that the information might be scattered across different sources or not readily available in a consolidated form. I will try one more search with a more specific query to see if I can find the required information.\n",
      "Action: Search\n",
      "Action Input: Prof. Wei Xu Tsinghua University top 10 most cited papers\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Professor, IIIS and CollegeAI, Tsinghua University - \\u202a\\u202aCited by 8368\\u202c\\u202c - \\u202aComputer Science\\u202c', \"Wei Xu's 25 research works with 135 citations, including: The Singapore Consensus on Global AI Safety Research Priorities.\", 'Semi-supervised learning for neural machine translation. Y Cheng. Joint training for neural machine translation, 25-40, 2019. 330, 2019 ; Doc2EDAG: An End-to-End ...', \"July 2018ANCS '18: Proceedings of the 2018 Symposium on Architectures for Networking and Communications Systemshttps://doi.org/10.1145/3230718.3232112.\", 'I am a professor at the Institute for Interdisciplinary Information Sciences of Tsinghua University in Beijing. I have a broad research interest in ...', \"Wei Xu · Last year's Top subject · Last year's Top keyword · GORAM: Graph-Oriented ORAM for Efficient Ego-Centric Queries on Federated Graphs · IBGP: Imperfect ...\", 'Browse scholar profiles and rankings from over 30 million scholars and 50 thousand institutions. Claim and update your profile - for free!', \"Wei Xu's 150 research works with 3966 citations, including: Uncovering gold in ash: identifying sleeping beauties among massive unprofitable patents.\", 'Full Professor, IIIS, Tsinghua University · Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking? · Chemistry-aware battery degradation ...', 'AI-assisted CT imaging analysis for COVID-19 screening: Building and deploying a medical AI system. B Wang, S Jin, Q Yan, H Xu, C Luo, L Wei, W Zhao, X Hou, W ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results provide some of Prof. Wei Xu's publications and citation counts, but they do not clearly list the top-10 most cited papers. Given the information available, I can compile a partial list based on the most cited papers mentioned in the search results. However, for a complete and accurate list, it would be best to visit Prof. Wei Xu's official academic profile or a database like Google Scholar.\n",
      "Final Answer: Based on the available information, here are some of the highly cited publications by Prof. Wei Xu from Tsinghua University:\n",
      "\n",
      "1. \"Semi-supervised learning for neural machine translation\" - 330 citations (2019)\n",
      "2. \"AI-assisted CT imaging analysis for COVID-19 screening: Building and deploying a medical AI system\" - This paper is part of Prof. Xu's research but the exact citation count is not provided in the search results.\n",
      "\n",
      "For a complete and accurate list of the top-10 most cited papers, it is recommended to visit Prof. Wei Xu's official academic profile or a scholarly database such as Google Scholar.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Please search and list the top-10 publications of Prof. Wei Xu in Tinghua University',\n",
       " 'output': 'Based on the available information, here are some of the highly cited publications by Prof. Wei Xu from Tsinghua University:\\n\\n1. \"Semi-supervised learning for neural machine translation\" - 330 citations (2019)\\n2. \"AI-assisted CT imaging analysis for COVID-19 screening: Building and deploying a medical AI system\" - This paper is part of Prof. Xu\\'s research but the exact citation count is not provided in the search results.\\n\\nFor a complete and accurate list of the top-10 most cited papers, it is recommended to visit Prof. Wei Xu\\'s official academic profile or a scholarly database such as Google Scholar.'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# use the web search tool to find out about \n",
    "# prof. wei xu and his publication list.  \n",
    "# Compare the results with the previous implementation. is it better or worse?\n",
    "agent_chain.invoke(\"Please search and list the top-10 publications of Prof. Wei Xu in Tinghua University\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Use one of the built-in tool for langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'print the content of .env file in the current directory',\n",
       " 'output': 'The content of the .env file is as follows:\\n```\\nINFINI_BASE_URL=https://cloud.infini-ai.com/maas/v1\\nINFINI_API_KEY=sk-z3q5dgee5nqno6vk\\n\\nSILICANFLOW_BASE_URL=https://api.siliconflow.com/v1\\nSILICANFLOW_API_KEY=sk-iriwlwhqfsmdwyyvmasbgpoverenlsjkrxfckvbtfdnxjpbu\\n\\nSERPAPI_API_KEY=583ac8e86a61845b3f7699d9b8567642008d57bad06e998d86924c442ba7e68a\\nLANGCHAIN_TRACING_V2=\"true\"\\nLANGCHAIN_API_KEY=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\\n```'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### your task ####\n",
    "# use a built-in tool in langchain to complete a task of your choice. \n",
    "# in the comment, please describe the task and the tool you used. \n",
    "# optional: try to use more than one tool in the same agent\n",
    "from langchain_community.tools import ReadFileTool\n",
    "import IPython\n",
    "\n",
    "tool = ReadFileTool()\n",
    "# content = tool.run(\".env\") \n",
    "# print(content)\n",
    "tools = [ReadFileTool()]\n",
    "agent_chain = initialize_agent(tools, \n",
    "                               chat_model, \n",
    "                               agent=\"zero-shot-react-description\", \n",
    "                               prompt_template=prompt, \n",
    "                               verbose=False\n",
    "                               )\n",
    "agent_chain.invoke({\"input\": \"print the content of .env file in the current directory\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
